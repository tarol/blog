---
title: 所谓编码--泛谈ASCII、Unicode、UTF8、UTF16、UCS-2等编码格式
date: 2017-09-15 18:18:38
tags:
  - 文章
  - 计算机
categories:
  - 技术
  - 计算机
---

最近在看 nodejs 的源码，看到 stream 的实现里面满地都是 encoding，不由想起以前看过的一篇文章——在前面的随笔里面有提到过——阮一峰老师的[《字符编码笔记:ASCII，Unicode 和 UTF-8》](http://www.ruanyifeng.com/blog/2007/10/ascii_unicode_and_utf-8.html)。

好的文章有一个好处，你每次看都会有新的收获，它就像一款拼图，你每次看都能收获几块碎片，补齐之前的认识；而好文章与拼图不一样的是，好文章是一块无垠的世界，当你不愿局限于当前的眼界的时候，你可以主动走出去，外面要更宽广、更精彩的多。

闲话说到这，开始聊聊所谓的编码。

大家都知道，计算机只认识 0 和 1，不认识什么 abc。如果想让计算机显示 abc，那么就要有一张 1 对 1 的表，用这张表告诉计算机，什么样的二进制串——比如（010101011100）——代表的是 a，什么样的串代表的是 b，这个表描述**二进制串**到**符号**的对应关系。ASCII 就是这么一张最早也是最简单的表，这张表简单到包含 128 个符号，比如 10 个数字、26 个小写字母、26 个大写字母，和一堆标点符号（如英文句号、逗号等）还有控制字符（如回车、tab 等）。那为什么是 128，不是 100 或 182？因为 128 正好用 7 个 bit（一个 0 或者 1 为一个 bit）表示。那么为什么不是 256 对或者更多？因为对于英语地区 128 个符号够用了，而在 ASCII 推出的那个年代（1967），大家还没开始着眼全球——这点从名字就可以看出来，ASCII = American Standard Code for Information Interchange（美国信息交换标准码），根本就没打算让别人上车。

然后其他语言地区很快入场了，而且随着八位机的普及，1 字节=8bit 成为了共识，大家都盯上了多出来的 128 个位置。这个时代群魔乱舞，基本是个公司就想染指这块标准，乱象直到 1985 年才通过 ISO/IEC 8859 把 EASCII 确定下来。

而在这个过程中，有一个地区的人根本就不跟你玩，就是意表文字地区，明白的说，主要是中日韩。开玩笑，256 个位置，你全让出来都不够我做两句诗。老司机不带怎么办，只能自己开车了，首先是日本站出来，于 1978 年出台了最早的汉字编码，然后中国大陆、中国台湾、韩国都在 80 年代出台了自己的汉字编码。这个时候，大家各自玩各自的没什么问题，聚在一起，问题就来了。比如一篇文章中包含中日韩三种文字，一串 01 的组合在中国的编码对应的是某个字，在日本的编码对应的却是另一个字，那计算机最后到底显示哪个字，计算机也很为难。有冲突怎么办，开个会通通气吧，于是大家坐在一起成立了个组织，叫 CJK-JRG（China, Japan, Korea Joint Research Group）。虽然这个组织折腾了很多年，而且最终提案也被否决了，但是为另一个方案提供了足够的信息，就是 Unicode。

Unicode 项目于 1987 年启动，在吸收了 CJK-JRG 的方案后于 1992 年 6 月份发布 1.0.1 版（之前的 1.0.0 没有包含汉字），迄今为止还在增修，最新的版本是 2017.6.20 公布的 10.0.0。最早的 Unicode 被设计为 16bit，即每个符号占 2byte，最多表示 65536 个符号。而后随着内容的增加，又基于原有设计不变的原则，将最早的 65536 个字符集合称为**基本多文种平面（Basic Multilingual Plane, BMP）**，并添加 16 个**辅助平面**（总共支持 65536 \* 17 = 1114112 个符号）。这样一来，原来的 16bit 就不够用了，需要 21bit 才能准确描述一个符号，相当于 3byte 不到，但是为了以后扩展方便及统一，辅助平面的符号要求使用 4byte 描述。

Unicode 解决了全世界人民用一套符号编码的问题，但却没有解决另一个问题，就是怎么存储的问题。按照一般的想法，所有的符号都必须以最长的 Unicode 符号的标准来存储，也就是 4byte，这样才不会有信息丢失。但是这样的话，对于全部是英文的文档，要浪费掉 3/4 的区域，对于大多数汉字，即 BMP 中的汉字，也要浪费掉 1/2 的区域。所以野蛮的使用 4byte 进行存储是不可取的，那么就要设计一套变长的规则来处理不同类型的符号，这时候 UTF8、UTF16 等就应运而生了，也就是说 UTF8、UTF16 是 Unicode 的一种实现方案（标准的说法，是 Unicode 字符编码五层模型的第三层，如果你对五层模型感兴趣，跳转[《刨根究底字符编码》](http://www.cnblogs.com/benbenalin/p/6882293.html)）。

先说 UTF8，UTF8 是完全变长的，占用 1-6byte，乍一看，怎么比直接用 4byte 存储还多出一半呢？其实占用 4byte 的情况是很少的，少到在几乎可以忽略不计，而 5、6byte 基于当前 16 个辅助平面的情况下还用不上。一般来说，**英文占用 1byte，中文占用 3byte**（CJK-JRG 最早提供给 Unicode 的 20000 多个符号位于位于 U+4E00–U+9FFF，这块区域的符号统一都占 3byte），所以一般来说使用 UTF8 可以节省 1/4 到 3/4 的存储区域。这样似乎解决了存储的问题，但却带来了另一个问题，即识别的问题。比如我给你 3byte 的二进制信息，告诉你这代表了一个字，那你肯定很快能知道是什么字，但我如果不告诉你字数呢，是一个字，两个字，还是三个字？你根本识别不出来这一串二进制是什么。这就是变长的方案需要解决的第二个问题，告诉读取方哪几个 byte 是一组的，UTF8 的规则很简单，我直接从阮老师的博客里搬运过来。

> 1. 对于单字节的符号，字节的第一位设为 0，后面 7 位为这个符号的 unicode 码。因此对于英语字母，UTF-8 编码和 ASCII 码是相同的。
> 2. 对于 n 字节的符号（n>1），第一个字节的前 n 位都设为 1，第 n+1 位设为 0，后面字节的前两位一律设为 10。剩下的没有提及的二进制位，全部为这个符号的 unicode 码。

| Unicode 符号范围（十六进制） | UTF-8 编码方式（二进制）                              |
| ---------------------------- | ----------------------------------------------------- |
| 0000 0000-0000 007F          | 0xxxxxxx                                              |
| 0000 0080-0000 07FF          | 110xxxxx 10xxxxxx                                     |
| 0000 0800-0000 FFFF          | 1110xxxx 10xxxxxx 10xxxxxx                            |
| 0001 0000-001F FFFF          | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx                   |
| 0020 0000-03FF FFFF          | 111110xx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx          |
| 0400 0000-7FFF FFFF          | 1111110x 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx 10xxxxxx |

简单的说，你收到很多个 byte 的二进制，从第一个 byte 开始读，数第一个 0 出现之前的 1，有几个 1 就代表前面几个 byte 是一组的，0 个 1 就代表当前的这个 byte 孤家寡人一个。然后跳过这个组的所有 byte，继续之前数 1 的环节。分好组后，按组找到上表右边的规则，把规则内 x 的位置保留下来，01 的位置全部扔掉（**01 代表的位置是 UTF8 的元数据，x 代表的位置才是 Unicode 的数据**），拼成新的二进制串，这个串就是 Unicode 了。举个栗子：

```
11100110 10001000 10010001 11100110 10011000 10101111 01110100 01100001 01110010 01101111 01101100
```

以上有 11 个 byte，我们从第一个 byte11100110 开始，数第一个 0 前面的 1 的数量，有 3 个 1，代表 3 个 byte 是一组的。然后我们跳过这 3 个，第四个 byte 是 11100110，继续数 1 得出有 3 个 1，然后又给这 3 个 byte 分组。跳过这三个，到了第 7 个 byte，这往后的 5 个 byte 都是以 0 开头，说明每个 byte 为 1 组。现在我们分好组了，有 7 个组，分别是

```
[11100110, 10001000, 10010001], [11100110, 10011000, 10101111], [01110100], [01100001], [01110010], [01101111], [01101100]
```

现在我们按组找到表右对应的行，第一组、第二组对应第五行，其他组对应第三行，我们把行内 x 对应的位置保留，10 的位置删除，得到新的数组

```
[0110, 001000, 010001], [0110, 011000, 101111], [1110100], [1100001], [1110010], [1101111], [1101100]
```

然后把组内的二进制串起来得到 Unicode

```
[0110001000010001], [0110011000101111], [1110100], [1110100], [1100001], [1110010], [1101111], [1101100]
```

这时候我们再按 byte 进行拆分以便阅读，并且在高位补 0

```
[01100010 00010001], [01100110 00101111], [01110100], [01110100], [01100001], [01110010], [01101111], [01101100]
```

再转换成 16 进制

```
[62 11], [66 2F], [74], [61], [72], [6F], [6C]
```

这时候我们打开 F12，在控制台输入对应的 Unicode（语法要求必须使用 4 位 16 进制数字）

```
'\u6211\u662f\u0074\u0061\u0072\u006f\u006c'
```

得到了对应的字符串“我是 tarol”。

好了，以上是 UTF8 的内容，之所以叫 UTF8 是因为这个规则下的符号，最少占 8bit。那么 UTF16 就好理解了，在这个规则下，每个符号最少占 16bit。UTF16 的规则说起来更简单，当符号位于 BMP 中时，占用 2byte，在符号位于辅助平面时，占用 4byte。那既然是变长的，又碰到了上面的问题，怎么识别这一块是 4byte 为一组还是 2byte 为一组？

这里就要提到 Unicode 的保留区块了，Unicode 规定从 U+D800 到 U+DFFF 之间是永久保留不赋予任何符号的。也就是正常情况下，2byte 如果落在这个范围内，那么就是 Unicode 的非法字节。而 UTF16 的做法就是把辅助平面的 Unicode 码进行处理，变成 4 个字节，并且两两落在非法区域内，读取方读到了非法字节，就可以界定这里是 4byte 为一组，不然就是 2byte 为一组。那么 UTF16 这个转换的算法又是怎样的呢？

1. 首先按现在 17 个平面的限制，辅助平面的码位是 U+10000 到 U+10FFFF，我们得到了一个辅助平面的 Unicode 码时，先减去 BMP 的码数 0x10000，得到的数介于 0 到 0xFFFFF 之间，最多用 20bit 表示
2. 然后我们把 20bit 从中间隔开，分为高位的 10bit 和低位的 10bit
3. 我们知道 10bit 的取值范围是 0 到 0x3FF，高位的 10bit 加上固定值 0xD800，得到的值叫做前导代理（lead surrogate），范围是 0xD800 到 0xDBFF
4. 低位的 10bit 加上固定值 0xDC00，得到的值叫做后尾代理（tail surrogate），范围是 0xDC00 到 0xDFFF。这样一来，不仅高位和低位都落在了保留区块内，而且彼此还做了区分。

还是举个例子。

𤭢，这个字是个异体字，通“碎”，位于辅助平面，Unicode 码位是 U+24B62，我们来算一下它的 UTF16 编码结果

1. 首先 0x24B62 减去 10000 得到 0x14B62，根据这 5 个 byte 得到 20bit，0001 0100 1011 0110 0010
2. 然后分成高位的 10bit（0001010010）和低位的 10bit（1101100010）
3. 高位+0xD800 得到（1101 1000 0101 0010）
4. 低位+0xDC00 得到（1101 1111 0110 0010）
5. 转换为 16 进制就是 0xD852 和 0xDF62，这就是 𤭢 的 UTF16 表示。

然后我们验证一下答案，照常打开控制台，键入 '𤭢'.charCodeAt(0).toString(16) 得到 'd852'，键入'𤭢'.charCodeAt(1).toString(16) 得到 'df62'，验证成功！而且这里还透露了一个细节，**ES 规定 string 是经过 UTF16 编码的**（[ES5 标准文档](https://www.w3.org/html/ig/zh/wiki/ES5/types#String.E7.B1.BB.E5.9E.8B)）。

UTF16 的事还没完，如果用过 nodejs 里面的 string_decoder 接口的人肯定注意到了，其中对 UTF16 编码的支持叫 utf16le，这个 le 是什么？其实这个是 Little Endian 的简称，对应的是 Big Endian。我们之前举的例子就是 Big Endian，Little Endian 不同在于每个 2byte 组里面的顺序是反过来的，即上面的 0xD852 和 0xDF62 改成 0x52D8 和 0x62DF 就是 𤭢 的 utf16le 编码了。至于为什么会有这么蛋疼的区分，那是操作系统的遗留问题，就像 window 的 CRLF 和 unix 的 LF 一样。

UTF16 告一段落了，新问题又来了。我们有 UTF8、UTF16LE、UTF16BE 这么多种编码，那一串二进制流过来我们用哪种编码方式去解析呢？尤其是 UTF16LE 和 UTF16BE，它们大部分规则是一样的，只是反过来了罢了。这里就要提到文件的一个元数据叫 **BOM**（byte-order mark）了，BOM 位于文件二进制流的最前方，标识当前文件的编码格式。UTF16LE 的 BOM 为 FF FE，UTF16BE 的 BOM 为 FE FF，UTF8 的 BOM 为 EF BB BF，但是一般不建议 UTF8 文件带 BOM。举个栗子，如果文件内容只有’0‘（十六进制编码为 30），那么三种编码方式生成的文件的十六进制编码分别为

| 编码 | 十六进制内容 |
| - | - |
| UTF8 | EF BB BF 30 |
| UTF16BE | FE FF 00 30 |
| UTF16LE | FF FE 00 30 |

从上面的例子也可以看到，UTF16 最大的问题在于：哪怕是 ASCII 标准字符 0，也占用了 2byte。这不仅仅浪费了存储空间，关键在于 UTF16 和 ASCII 不兼容，比如我新建一个文件，内容为 1234567890，使用系统自带的记事本打开，再另存为 Unicode 编码（就是 UTF16LE）

![1](/assets/image/charset/1.png)
![2](/assets/image/charset/2.png)

然后再选择打开，选择当前文件，再 ANSI 编码

![3](/assets/image/charset/3.png)
![4](/assets/image/charset/4.png)
![5](/assets/image/charset/5.png)

你会看到这个样子的内容

![6](/assets/image/charset/6.png)

可以发现，不仅信息错乱了，每个数字之间还有空格。

本来这篇文章到此就结束了，直到我在 nodepad++里面看到了这个

![7](/assets/image/charset/7.png)

UCS-2 是什么鬼，好像在哪里见过？瞬间脑子里像侦探回忆线索般闪过画面，后来整理发现这就是 UTF16 的 low 版。为什么说是 low 版，因为 USC-2 是定长的，是不支持辅助平面的 UTF16。我们验证下，把‘𤭢’复制到用 notepad++打开的以 UCS-2 编码的文件里，没看出有什么问题，这时候关闭 notepad++再打开。

![8](/assets/image/charset/8.png)

可以看到，‘𤭢’字碎的只剩渣了。
